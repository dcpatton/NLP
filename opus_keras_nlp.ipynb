{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41147537",
   "metadata": {},
   "source": [
    "# Translation of OPUS medical text from German to English\n",
    "https://www.tensorflow.org/datasets/catalog/opus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc452da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_nlp -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894bae8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "4.7.0\n",
      "0.3.1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n",
    "import keras_nlp\n",
    "tfds.disable_progress_bar()\n",
    "print(tf.__version__)\n",
    "print(tfds.__version__)\n",
    "print(keras_nlp.__version__)\n",
    "seed = 54\n",
    "random.seed(54)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6ddd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load('opus', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "855ac4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "for de, en in dataset['train']:\n",
    "    german = de.numpy().decode().lower().strip('\\n')\n",
    "    english = en.numpy().decode().lower().strip('\\n')\n",
    "    text_pairs.append((german, english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a491c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('es werden möglicherweise nicht alle packungsgrößen in den verkehr gebracht.', 'not all pack sizes may be marketed.')\n",
      "\n",
      "('benutzen sie die durchstechflaschen nicht, wenn die verschlusskappen locker sind oder fehlen.', 'if the caps are loose or missing, do not use the vials.')\n",
      "\n",
      "('3.', '63 3.')\n",
      "\n",
      "('sie dürfen das arzneimittel nach dem auf dem etikett und dem umkarton angegebenen verfalldatum nicht mehr anwenden.', 'do not use mixtard after the expiry date which is stated on the label and the carton.')\n",
      "\n",
      "('32,4 (78/241) 51,3 (40/78) 36,7; 65,9', '32.4 (78/241)')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682cd01",
   "metadata": {},
   "source": [
    "**Looks like we have some questionable data. In the future I may remove any pairs that don't have any letters for the alphabet in them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59f2857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1108752 total pairs\n",
      "997878 training pairs\n",
      "55437 validation pairs\n",
      "55437 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.05 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e593a",
   "metadata": {},
   "source": [
    "# Tokenize the data\n",
    "\n",
    "**Note: In this notebook, I use the prefix 'de' for German and 'en' for English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b87dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    bert_vocab_args = dict(vocab_size=vocab_size, reserved_tokens=reserved_tokens, \n",
    "                           bert_tokenizer_params={\"lower_case\": True},)\n",
    "\n",
    "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = bert_vocab_from_dataset.bert_vocab_from_dataset(word_piece_ds.batch(1000).prefetch(tf.data.AUTOTUNE), \n",
    "                                                            **bert_vocab_args)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8bef45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 15000\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "de_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "de_vocab = train_word_piece(de_samples, VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "en_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "en_vocab = train_word_piece(en_samples, VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56863fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(vocabulary=de_vocab, lowercase=False)\n",
    "en_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(vocabulary=en_vocab, lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbed1b0",
   "metadata": {},
   "source": [
    "# Format the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c28b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 64\n",
    "def preprocess_batch(de, en):\n",
    "    batch_size = tf.shape(en)[0]\n",
    "\n",
    "    de = de_tokenizer(de)\n",
    "    en = en_tokenizer(en)\n",
    "\n",
    "    # Pad `de` to `MAX_SEQUENCE_LENGTH`.\n",
    "    de_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=de_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    de = de_start_end_packer(de)\n",
    "\n",
    "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `en` and pad it as well.\n",
    "    en_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=en_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=en_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=en_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    en = en_start_end_packer(en)\n",
    "\n",
    "    return ({\"encoder_inputs\": de, \"decoder_inputs\": en[:, :-1],}, en[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs, shuffle=False):\n",
    "    de_texts, en_texts = zip(*pairs)\n",
    "    de_texts = list(de_texts)\n",
    "    en_texts = list(en_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((de_texts, en_texts))\n",
    "    dataset = dataset.batch(64)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(2048)\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs, shuffle=True)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e9d33",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b471d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dcpat\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    "    )(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    "    )(inputs=x)\n",
    "\n",
    "encoder = tf.keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = tf.keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    "    )(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    "    )(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = tf.keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "\n",
    "decoder = tf.keras.Model([decoder_inputs, encoded_seq_inputs,], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bac2c8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89dbc371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15592/15592 [==============================] - 3881s 249ms/step - loss: 0.5366 - acc: 0.6334 - val_loss: 0.3114 - val_acc: 0.7625\n",
      "Epoch 2/10\n",
      "15592/15592 [==============================] - 3835s 246ms/step - loss: 0.3090 - acc: 0.7712 - val_loss: 0.2542 - val_acc: 0.8037\n",
      "Epoch 3/10\n",
      "15592/15592 [==============================] - 3860s 247ms/step - loss: 0.2607 - acc: 0.8032 - val_loss: 0.2311 - val_acc: 0.8219\n",
      "Epoch 4/10\n",
      "15592/15592 [==============================] - 3814s 244ms/step - loss: 0.2342 - acc: 0.8211 - val_loss: 0.2152 - val_acc: 0.8342\n",
      "Epoch 5/10\n",
      "15592/15592 [==============================] - 3786s 243ms/step - loss: 0.2165 - acc: 0.8331 - val_loss: 0.2065 - val_acc: 0.8416\n",
      "Epoch 6/10\n",
      "15592/15592 [==============================] - 3774s 242ms/step - loss: 0.2036 - acc: 0.8420 - val_loss: 0.1995 - val_acc: 0.8469\n",
      "Epoch 7/10\n",
      "15592/15592 [==============================] - 3773s 242ms/step - loss: 0.1937 - acc: 0.8488 - val_loss: 0.1965 - val_acc: 0.8510\n",
      "Epoch 8/10\n",
      "15592/15592 [==============================] - 3772s 242ms/step - loss: 0.1859 - acc: 0.8542 - val_loss: 0.1918 - val_acc: 0.8550\n",
      "Epoch 9/10\n",
      "15592/15592 [==============================] - 3773s 242ms/step - loss: 0.1796 - acc: 0.8586 - val_loss: 0.1894 - val_acc: 0.8566\n",
      "Epoch 10/10\n",
      "15592/15592 [==============================] - 3808s 244ms/step - loss: 0.1742 - acc: 0.8623 - val_loss: 0.1864 - val_acc: 0.8589\n"
     ]
    }
   ],
   "source": [
    "transformer.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                    loss=\"sparse_categorical_crossentropy\",\n",
    "                    metrics=['acc'])\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=9, verbose=1, restore_best_weights=True)\n",
    "history = transformer.fit(train_ds, epochs=10, validation_data=val_ds, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18aed9",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2bd600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example 0 **\n",
      "\n",
      "90/158 (57%)\n",
      "\n",
      "90 / 158 ( 57 % )\n",
      "\n",
      "** Example 1 **\n",
      "\n",
      "η έκδοση γνώμης από την chmp απαιτεί κατά κανόνα έως και 90 ημέρες, μετά την παραλαβή μιας αίτησης για τροποποίηση της άδειας κυκλοφορίας.\n",
      "\n",
      "the chmp was also considered the matter to be in addition to the chmp .\n",
      "\n",
      "** Example 2 **\n",
      "\n",
      "roche austria gmbh tel: +43 (0) 1 27739\n",
      "\n",
      "osterreich roche austria gmbh tel : + 43 ( 0 ) 1 27739\n",
      "\n",
      "** Example 3 **\n",
      "\n",
      "ziehen sie den kolben der spritze langsam zurück, bis das wasser die 1,1-ml-markierung erreicht.\n",
      "\n",
      "slowly pull the plunger to the water to the 1 . 1 ml mark .\n",
      "\n",
      "** Example 4 **\n",
      "\n",
      "1,9%; placebo:\n",
      "\n",
      "1 . 9 % ; placebo :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    # Tokenize the encoder input.\n",
    "    encoder_input_tokens = de_tokenizer(input_sentences).to_tensor(\n",
    "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(decoder_input_tokens):\n",
    "        return transformer([encoder_input_tokens, decoder_input_tokens])[:, -1, :]\n",
    "\n",
    "    # Set the prompt to the \"[START]\" token.\n",
    "    prompt = tf.fill((batch_size, 1), en_tokenizer.token_to_id(\"[START]\"))\n",
    "\n",
    "    generated_tokens = keras_nlp.utils.greedy_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=40,\n",
    "        end_token_id=en_tokenizer.token_to_id(\"[END]\"),\n",
    "    )\n",
    "    generated_sentences = en_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences\n",
    "\n",
    "\n",
    "test_de_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(5):\n",
    "    input_sentence = random.choice(test_de_texts)\n",
    "    translated = decode_sequences(tf.constant([input_sentence]))\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"** Example {i} **\")\n",
    "    print()\n",
    "    print(input_sentence)\n",
    "    print()\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6a82c",
   "metadata": {},
   "source": [
    "# Quantitative Evaluation\n",
    "\n",
    "ROUGE-N is a score based on the number of matching n-grams between the reference text and the hypothesis text. ROUGE-1, ROUGE-2, and ROUGE-3 use the number of common unigrams, bigrams, and trigrams, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20ffcb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "369d4154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Score:  0.86314577\n",
      "ROUGE-2 Score:  0.62874866\n",
      "ROUGE-3 Score:  0.50575006\n",
      "Wall time: 51.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rouge_1 = keras_nlp.metrics.RougeN(order=1)\n",
    "rouge_2 = keras_nlp.metrics.RougeN(order=2)\n",
    "rouge_3 = keras_nlp.metrics.RougeN(order=3)\n",
    "\n",
    "for test_pair in test_pairs[:30]:  ## just evaluating the first 30 because this is a very slow calculation\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "\n",
    "    translated_sentence = decode_sequences(tf.constant([input_sentence]))\n",
    "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
    "    translated_sentence = (\n",
    "        translated_sentence.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    rouge_1(reference_sentence, translated_sentence)\n",
    "    rouge_2(reference_sentence, translated_sentence)\n",
    "    rouge_3(reference_sentence, translated_sentence)\n",
    "\n",
    "print(\"ROUGE-1 Score: \", rouge_1.result()['f1_score'].numpy())\n",
    "print(\"ROUGE-2 Score: \", rouge_2.result()['f1_score'].numpy())\n",
    "print(\"ROUGE-3 Score: \", rouge_3.result()['f1_score'].numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
